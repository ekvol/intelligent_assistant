
# Установка зависимостей (запустить только один раз)
!pip install transformers torch pandas matplotlib scikit-learn

# Импорт библиотек
import pandas as pd
import torch
from torch.utils.data import Dataset, DataLoader
from transformers import T5Tokenizer, T5ForConditionalGeneration, AdamW
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split

# Проверка CUDA
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Используемое устройство:", device)

# Загрузка датасета
df = pd.read_csv("data/sample_data.csv")
df = df.dropna()
df = df.head(50)  # Для ускорения — 50 примеров

# Формат входа для T5: task prefix + input_text
df["input_text"] = "translate Russian to English: " + df["question"]
df["target_text"] = df["answer"]

# Токенизатор и модель
tokenizer = T5Tokenizer.from_pretrained("t5-small")
model = T5ForConditionalGeneration.from_pretrained("t5-small").to(device)

# Кастомный датасет
class QADataset(Dataset):
    def __init__(self, inputs, targets, tokenizer, max_len=32):
        self.inputs = inputs
        self.targets = targets
        self.tokenizer = tokenizer
        self.max_len = max_len

    def __len__(self):
        return len(self.inputs)

    def __getitem__(self, idx):
        input = self.tokenizer(self.inputs[idx], truncation=True, padding="max_length",
                               max_length=self.max_len, return_tensors="pt")
        target = self.tokenizer(self.targets[idx], truncation=True, padding="max_length",
                                max_length=self.max_len, return_tensors="pt")
        return {
            "input_ids": input["input_ids"].squeeze(),
            "attention_mask": input["attention_mask"].squeeze(),
            "labels": target["input_ids"].squeeze()
        }

# Разделение на обучающую и тестовую выборку
train_texts, val_texts, train_labels, val_labels = train_test_split(
    df["input_text"].tolist(), df["target_text"].tolist(), test_size=0.2
)

train_dataset = QADataset(train_texts, train_labels, tokenizer)
val_dataset = QADataset(val_texts, val_labels, tokenizer)

train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=4)

# Обучение модели
optimizer = AdamW(model.parameters(), lr=5e-5)
epochs = 5
train_losses = []

model.train()
for epoch in range(epochs):
    total_loss = 0
    for batch in train_loader:
        input_ids = batch["input_ids"].to(device)
        attention_mask = batch["attention_mask"].to(device)
        labels = batch["labels"].to(device)

        outputs = model(input_ids=input_ids,
                        attention_mask=attention_mask,
                        labels=labels)

        loss = outputs.loss
        total_loss += loss.item()

        loss.backward()
        optimizer.step()
        optimizer.zero_grad()

    avg_loss = total_loss / len(train_loader)
    train_losses.append(avg_loss)
    print(f"Эпоха {epoch+1}, Средняя потеря: {avg_loss:.4f}")

# Сохраняем модель
torch.save(model.state_dict(), "model/model.pt")
print("✅ Модель сохранена: model/model.pt")

# Визуализация потерь
plt.plot(train_losses, marker='o')
plt.title("Потери на обучении")
plt.xlabel("Эпоха")
plt.ylabel("Loss")
plt.grid()
plt.show()

# Тестирование на одном примере
model.eval()
sample_input = val_texts[0]
input_ids = tokenizer(sample_input, return_tensors="pt", max_length=32,
                      truncation=True, padding="max_length").input_ids.to(device)
output_ids = model.generate(input_ids, max_length=32)
print("❓ Вопрос:", sample_input)
print("✅ Ответ:", tokenizer.decode(output_ids[0], skip_special_tokens=True))
