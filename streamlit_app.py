
import streamlit as st
import torch
from transformers import T5Tokenizer, T5ForConditionalGeneration

# –ù–∞—Å—Ç—Ä–æ–π–∫–∏ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
st.set_page_config(page_title="–ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã–π –ø–æ–º–æ—â–Ω–∏–∫", layout="centered")

# –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏
@st.cache_resource
def load_model():
    tokenizer = T5Tokenizer.from_pretrained("t5-small")
    model = T5ForConditionalGeneration.from_pretrained("t5-small")
    model.load_state_dict(torch.load("model/model.pt", map_location=torch.device("cpu")))
    model.eval()
    return tokenizer, model

tokenizer, model = load_model()

# –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞
def generate_answer(question):
    input_text = f"translate Russian to English: {question}"
    input_ids = tokenizer.encode(input_text, return_tensors="pt", max_length=32, truncation=True)
    output_ids = model.generate(input_ids, max_length=32)
    return tokenizer.decode(output_ids[0], skip_special_tokens=True)

# –ò–Ω—Ç–µ—Ä—Ñ–µ–π—Å
st.title("üéì –ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã–π –ø–æ–º–æ—â–Ω–∏–∫ –¥–ª—è –∞–±–∏—Ç—É—Ä–∏–µ–Ω—Ç–æ–≤")
question = st.text_input("–í–≤–µ–¥–∏—Ç–µ –≤–∞—à –≤–æ–ø—Ä–æ—Å –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ:")

if question:
    with st.spinner("–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞..."):
        answer = generate_answer(question)
    st.success(f"–û—Ç–≤–µ—Ç: {answer}")
